{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TxGNN Subgraph Training & Evaluation\n",
    "\n",
    "This notebook reproduces what the Python script `run_txgnn.py` does, but lets you run and display results directly here for your progress report.\n",
    "\n",
    "Make sure your environment has **TxGNN**, **DGL 0.5.2**, and **PyTorch ≥ 2.2** installed, and that your dataset folder (e.g. `data/subgraphs/drug-disease-gene/`) is already prepared using `prune_subgraph.py` and `prepare_split.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and environment check ---\n",
    "import torch\n",
    "from txgnn import TxData, TxGNN, TxEval\n",
    "import os, sys\n",
    "\n",
    "print('Python version:', sys.version)\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "\n",
    "# Path to one of your subgraph folders (edit as needed)\n",
    "DATA_DIR = './data/subgraphs/drug-disease-gene'\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Seeds and save paths\n",
    "SEED = 42\n",
    "SAVE_MODEL_PATH = './finetune_subgraph.pt'\n",
    "SAVE_EVAL_PATH = './eval_subgraph'\n",
    "\n",
    "# Model dimensions (small for Colab / CPU)\n",
    "N_HID, N_INP, N_OUT = 64, 64, 64\n",
    "PROTO, PROTO_NUM = True, 2\n",
    "ATTN = False\n",
    "SIM_MEASURE = 'all_nodes_profile'\n",
    "AGG_MEASURE = 'rarity'\n",
    "NUM_WALKS, PATH_LEN = 50, 2\n",
    "\n",
    "# Training schedule\n",
    "PRETRAIN_EPOCHS = 1      # 0 to skip pretrain\n",
    "PRETRAIN_LR = 1e-3\n",
    "PRETRAIN_BATCH = 512\n",
    "FINETUNE_EPOCHS = 30\n",
    "FINETUNE_LR = 5e-4\n",
    "TRAIN_PRINT_N = 10\n",
    "VALID_EVERY_N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data and prepare split ---\n",
    "print('Loading data from:', DATA_DIR)\n",
    "TxDataObj = TxData(data_folder_path=DATA_DIR)\n",
    "TxDataObj.prepare_split(split='full_graph', seed=SEED)\n",
    "print('Data ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize model ---\n",
    "Tx = TxGNN(\n",
    "    data=TxDataObj,\n",
    "    weight_bias_track=False,\n",
    "    proj_name='TxGNN',\n",
    "    exp_name='subgraph_notebook',\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "Tx.model_initialize(\n",
    "    n_hid=N_HID,\n",
    "    n_inp=N_INP,\n",
    "    n_out=N_OUT,\n",
    "    proto=PROTO,\n",
    "    proto_num=PROTO_NUM,\n",
    "    attention=ATTN,\n",
    "    sim_measure=SIM_MEASURE,\n",
    "    agg_measure=AGG_MEASURE,\n",
    "    num_walks=NUM_WALKS,\n",
    "    path_length=PATH_LEN,\n",
    ")\n",
    "print('Model initialized on', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional pretraining ---\n",
    "if PRETRAIN_EPOCHS > 0:\n",
    "    print('Starting pretraining...')\n",
    "    Tx.pretrain(\n",
    "        n_epoch=PRETRAIN_EPOCHS,\n",
    "        learning_rate=PRETRAIN_LR,\n",
    "        batch_size=PRETRAIN_BATCH,\n",
    "        train_print_per_n=TRAIN_PRINT_N,\n",
    "    )\n",
    "else:\n",
    "    print('Skipping pretrain step.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finetune on drug–disease prediction ---\n",
    "print('Starting finetune...')\n",
    "Tx.finetune(\n",
    "    n_epoch=FINETUNE_EPOCHS,\n",
    "    learning_rate=FINETUNE_LR,\n",
    "    train_print_per_n=TRAIN_PRINT_N,\n",
    "    valid_per_n=VALID_EVERY_N,\n",
    "    save_name=SAVE_MODEL_PATH,\n",
    ")\n",
    "print('Finetune complete. Model saved to', SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate ---\n",
    "print('Evaluating...')\n",
    "TxE = TxEval(model=Tx)\n",
    "results = TxE.eval_disease_centric(\n",
    "    disease_idxs='test_set',\n",
    "    show_plot=False,\n",
    "    verbose=True,\n",
    "    save_result=True,\n",
    "    return_raw=False,\n",
    "    save_name=SAVE_EVAL_PATH,\n",
    ")\n",
    "print('\\nEvaluation summary:')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Notes\n",
    "- You can re-run this notebook for different subgraph folders by changing `DATA_DIR` at the top.\n",
    "- The printed output and metrics will remain visible in the `.ipynb` file after saving, so your professor can see the training/evaluation logs.\n",
    "- For faster testing on CPU, reduce `FINETUNE_EPOCHS` or skip pretraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
